{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1436a0-3357-4850-b507-a12c76e60c22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Chapter 5: Advanced Operations in Spark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c029f8c-dfbc-4e10-b09d-ccbea7b62eec",
     "showTitle": true,
     "title": "Create Salary dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Employee: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|    John| Field-eng|  3500|\n| Michael| Field-eng|  4500|\n|  Robert|      NULL|  4000|\n|   Maria|   Finance|  3500|\n|    John|     Sales|  3000|\n|   Kelly|   Finance|  3500|\n|    Kate|   Finance|  3000|\n|  Martin|      NULL|  3500|\n|   Kiran|     Sales|  2200|\n| Michael| Field-eng|  4500|\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data = [(\"John\", \"Field-eng\", 3500), \n",
    "    (\"Michael\", \"Field-eng\", 4500), \n",
    "    (\"Robert\", None, 4000), \n",
    "    (\"Maria\", \"Finance\", 3500), \n",
    "    (\"John\", \"Sales\", 3000), \n",
    "    (\"Kelly\", \"Finance\", 3500), \n",
    "    (\"Kate\", \"Finance\", 3000), \n",
    "    (\"Martin\", None, 3500), \n",
    "    (\"Kiran\", \"Sales\", 2200), \n",
    "    (\"Michael\", \"Field-eng\", 4500) \n",
    "  ]\n",
    "columns= [\"Employee\", \"Department\", \"Salary\"]\n",
    "salary_data = spark.createDataFrame(data = salary_data, schema = columns)\n",
    "salary_data.printSchema()\n",
    "salary_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c64523b-97f8-4cdf-8c73-13723a7f7453",
     "showTitle": true,
     "title": "Using Groupby in a Dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [Department], value: [Employee: string, Department: string, Salary: bigint], type: GroupBy]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_data.groupby('Department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e2c600-8160-4138-968f-835e6757f06c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|Department|       avg(Salary)|\n+----------+------------------+\n| Field-eng| 4166.666666666667|\n|     Sales|            2600.0|\n|      NULL|            3750.0|\n|   Finance|3333.3333333333335|\n+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data.groupby('Department').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d437c9f0-2336-4687-83b4-7c8142b4085f",
     "showTitle": true,
     "title": "Complex Groupby Statement"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|Department|Salary|\n+----------+------+\n|      NULL|  7500|\n| Field-eng| 12500|\n|   Finance| 10000|\n|     Sales|  5200|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "salary_data.groupBy('Department')\\\n",
    "  .sum('Salary')\\\n",
    "  .withColumn('sum(Salary)',round(col('sum(Salary)'), 2))\\\n",
    "  .withColumnRenamed('sum(Salary)', 'Salary')\\\n",
    "  .orderBy('Department')\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc73dea-aa0c-4a54-aded-a4c3814f01a9",
     "showTitle": true,
     "title": "Joining Dataframes in Spark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n| ID|Employee|Department|Salary|\n+---+--------+----------+------+\n|  1|    John| Field-eng|  3500|\n|  2|  Robert|     Sales|  4000|\n|  3|   Maria|   Finance|  3500|\n|  4| Michael|     Sales|  3000|\n|  5|   Kelly|   Finance|  3500|\n|  6|    Kate|   Finance|  3000|\n|  7|  Martin|   Finance|  3500|\n|  8|   Kiran|     Sales|  2200|\n+---+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n",
    "    (2, \"Robert\", \"Sales\", 4000), \\\n",
    "    (3, \"Maria\", \"Finance\", 3500), \\\n",
    "    (4, \"Michael\", \"Sales\", 3000), \\\n",
    "    (5, \"Kelly\", \"Finance\", 3500), \\\n",
    "    (6, \"Kate\", \"Finance\", 3000), \\\n",
    "    (7, \"Martin\", \"Finance\", 3500), \\\n",
    "    (8, \"Kiran\", \"Sales\", 2200), \\\n",
    "  ]\n",
    "columns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
    "salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\n",
    "salary_data_with_id.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125e73d8-c716-4e1c-8900-859c1ec666e9",
     "showTitle": true,
     "title": "Employee data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| ID|State|Gender|\n+---+-----+------+\n|  1|   NY|     M|\n|  2|   NC|     M|\n|  3|   NY|     F|\n|  4|   TX|     M|\n|  5|   NY|     F|\n|  6|   AZ|     F|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_data = [(1, \"NY\", \"M\"), \\\n",
    "    (2, \"NC\", \"M\"), \\\n",
    "    (3, \"NY\", \"F\"), \\\n",
    "    (4, \"TX\", \"M\"), \\\n",
    "    (5, \"NY\", \"F\"), \\\n",
    "    (6, \"AZ\", \"F\") \\\n",
    "  ]\n",
    "columns= [\"ID\", \"State\", \"Gender\"]\n",
    "employee_data = spark.createDataFrame(data = employee_data, schema = columns)\n",
    "employee_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0137bf4-d318-4417-86ca-df79f2fb80be",
     "showTitle": true,
     "title": "Inner join"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+-----+------+\n| ID|Employee|Department|Salary| ID|State|Gender|\n+---+--------+----------+------+---+-----+------+\n|  1|    John| Field-eng|  3500|  1|   NY|     M|\n|  2|  Robert|     Sales|  4000|  2|   NC|     M|\n|  3|   Maria|   Finance|  3500|  3|   NY|     F|\n|  4| Michael|     Sales|  3000|  4|   TX|     M|\n|  5|   Kelly|   Finance|  3500|  5|   NY|     F|\n|  6|    Kate|   Finance|  3000|  6|   AZ|     F|\n+---+--------+----------+------+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34ff657-b0dd-4485-96f0-6d7c6126a1bd",
     "showTitle": true,
     "title": "Outer join"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+----+-----+------+\n| ID|Employee|Department|Salary|  ID|State|Gender|\n+---+--------+----------+------+----+-----+------+\n|  1|    John| Field-eng|  3500|   1|   NY|     M|\n|  2|  Robert|     Sales|  4000|   2|   NC|     M|\n|  3|   Maria|   Finance|  3500|   3|   NY|     F|\n|  4| Michael|     Sales|  3000|   4|   TX|     M|\n|  5|   Kelly|   Finance|  3500|   5|   NY|     F|\n|  6|    Kate|   Finance|  3000|   6|   AZ|     F|\n|  7|  Martin|   Finance|  3500|NULL| NULL|  NULL|\n|  8|   Kiran|     Sales|  2200|NULL| NULL|  NULL|\n+---+--------+----------+------+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868ca315-ab44-4eb6-b8f1-92481d770911",
     "showTitle": true,
     "title": "Left join"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+----+-----+------+\n| ID|Employee|Department|Salary|  ID|State|Gender|\n+---+--------+----------+------+----+-----+------+\n|  1|    John| Field-eng|  3500|   1|   NY|     M|\n|  2|  Robert|     Sales|  4000|   2|   NC|     M|\n|  3|   Maria|   Finance|  3500|   3|   NY|     F|\n|  4| Michael|     Sales|  3000|   4|   TX|     M|\n|  5|   Kelly|   Finance|  3500|   5|   NY|     F|\n|  6|    Kate|   Finance|  3000|   6|   AZ|     F|\n|  7|  Martin|   Finance|  3500|NULL| NULL|  NULL|\n|  8|   Kiran|     Sales|  2200|NULL| NULL|  NULL|\n+---+--------+----------+------+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cba2965-54b3-4d04-a456-77e9d9af6e1f",
     "showTitle": true,
     "title": "Right join"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+---+-----+------+\n| ID|Employee|Department|Salary| ID|State|Gender|\n+---+--------+----------+------+---+-----+------+\n|  1|    John| Field-eng|  3500|  1|   NY|     M|\n|  2|  Robert|     Sales|  4000|  2|   NC|     M|\n|  3|   Maria|   Finance|  3500|  3|   NY|     F|\n|  4| Michael|     Sales|  3000|  4|   TX|     M|\n|  5|   Kelly|   Finance|  3500|  5|   NY|     F|\n|  6|    Kate|   Finance|  3000|  6|   AZ|     F|\n+---+--------+----------+------+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9f95c1-4109-4ceb-925d-7b10cf838fdd",
     "showTitle": true,
     "title": "Union"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ID: long (nullable = true)\n |-- Employee: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n|1  |John    |Field-eng |3500  |\n|2  |Robert  |Sales     |4000  |\n|3  |Aliya   |Finance   |3500  |\n|4  |Nate    |Sales     |3000  |\n+---+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id_2 = [(1, \"John\", \"Field-eng\", 3500), \\\n",
    "    (2, \"Robert\", \"Sales\", 4000), \\\n",
    "    (3, \"Aliya\", \"Finance\", 3500), \\\n",
    "    (4, \"Nate\", \"Sales\", 3000), \\\n",
    "  ]\n",
    "columns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
    "\n",
    "salary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_id_2, schema = columns2)\n",
    "\n",
    "salary_data_with_id_2.printSchema()\n",
    "salary_data_with_id_2.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb3d433-2a89-47b4-9d21-2d79194809c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n|1  |John    |Field-eng |3500  |\n|2  |Robert  |Sales     |4000  |\n|3  |Maria   |Finance   |3500  |\n|4  |Michael |Sales     |3000  |\n|5  |Kelly   |Finance   |3500  |\n|6  |Kate    |Finance   |3000  |\n|7  |Martin  |Finance   |3500  |\n|8  |Kiran   |Sales     |2200  |\n|1  |John    |Field-eng |3500  |\n|2  |Robert  |Sales     |4000  |\n|3  |Aliya   |Finance   |3500  |\n|4  |Nate    |Sales     |3000  |\n+---+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "unionDF = salary_data_with_id.union(salary_data_with_id_2)\n",
    "unionDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d84b0031-6f62-41a8-9533-b510a487ab0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading and Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c8eb85-7d75-4010-977d-370b7940b57e",
     "showTitle": true,
     "title": "Reading and writing CSV files"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n| ID|Employee|Department|Salary|\n+---+--------+----------+------+\n|  1|    John| Field-eng|  3500|\n|  2|  Robert|     Sales|  4000|\n|  3|   Maria|   Finance|  3500|\n|  4| Michael|     Sales|  3000|\n|  5|   Kelly|   Finance|  3500|\n|  6|    Kate|   Finance|  3000|\n|  7|  Martin|   Finance|  3500|\n|  8|   Kiran|     Sales|  2200|\n+---+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "salary_data_with_id.write.csv('salary_data.csv', mode='overwrite', header=True)\n",
    "spark.read.csv('/salary_data.csv', header=True).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b033bc47-7a90-4ae1-b37b-692860e06482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n| ID|  State|   Gender|\n+---+-------+---------+\n|  1|   John|Field-eng|\n|  2| Robert|    Sales|\n|  3|  Maria|  Finance|\n|  4|Michael|    Sales|\n|  5|  Kelly|  Finance|\n|  6|   Kate|  Finance|\n|  7| Martin|  Finance|\n|  8|  Kiran|    Sales|\n+---+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "filePath = '/salary_data.csv'\n",
    "columns= [\"ID\", \"State\", \"Gender\"] \n",
    "schema = StructType([\n",
    "      StructField(\"ID\", IntegerType(),True),\n",
    "  StructField(\"State\",  StringType(),True),\n",
    "  StructField(\"Gender\",  StringType(),True)\n",
    "])\n",
    " \n",
    "read_data = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(filePath)\n",
    "read_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd8f639-d141-48c9-be8e-dffd764aa0ee",
     "showTitle": true,
     "title": "Reading and writing Parquet files"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n| ID|Employee|Department|Salary|\n+---+--------+----------+------+\n|  5|   Kelly|   Finance|  3500|\n|  6|    Kate|   Finance|  3000|\n|  1|    John| Field-eng|  3500|\n|  2|  Robert|     Sales|  4000|\n|  3|   Maria|   Finance|  3500|\n|  4| Michael|     Sales|  3000|\n|  7|  Martin|   Finance|  3500|\n|  8|   Kiran|     Sales|  2200|\n+---+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id.write.parquet('salary_data.parquet', mode='overwrite')\n",
    "spark.read.parquet('/salary_data.parquet').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492b344b-3719-44cd-a8dc-034d20f3a409",
     "showTitle": true,
     "title": "Reading and writing ORC files"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n| ID|Employee|Department|Salary|\n+---+--------+----------+------+\n|  5|   Kelly|   Finance|  3500|\n|  6|    Kate|   Finance|  3000|\n|  1|    John| Field-eng|  3500|\n|  2|  Robert|     Sales|  4000|\n|  7|  Martin|   Finance|  3500|\n|  8|   Kiran|     Sales|  2200|\n|  3|   Maria|   Finance|  3500|\n|  4| Michael|     Sales|  3000|\n+---+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id.write.orc('salary_data.orc', mode='overwrite')\n",
    "spark.read.orc('/salary_data.orc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3c1309-4a00-4a92-ac3e-7f2a9d491445",
     "showTitle": true,
     "title": "Reading and writing Delta files"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n| ID|Employee|Department|Salary|\n+---+--------+----------+------+\n|  1|    John| Field-eng|  3500|\n|  2|  Robert|     Sales|  4000|\n|  3|   Maria|   Finance|  3500|\n|  4| Michael|     Sales|  3000|\n|  5|   Kelly|   Finance|  3500|\n|  6|    Kate|   Finance|  3000|\n|  7|  Martin|   Finance|  3500|\n|  8|   Kiran|     Sales|  2200|\n+---+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id.write.format(\"delta\").save(\"/FileStore/tables/salary_data_with_id\", mode='overwrite')\n",
    "df = spark.read.load(\"/FileStore/tables/salary_data_with_id\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d616d17f-7848-4527-aae3-78eec9d3214d",
     "showTitle": true,
     "title": "Using SQL in Spark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|       8|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data_with_id.createOrReplaceTempView(\"SalaryTable\")\n",
    "spark.sql(\"SELECT count(*) from SalaryTable\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f549a552-a92a-477c-bcbd-0eaf6104c207",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66004b0-07ac-4c06-966e-1370a2e1b3d6",
     "showTitle": true,
     "title": "Catalyst Optimizer in Action"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(1) Project [employee#129490, department#129491]\n+- *(1) Filter (isnotnull(salary#129492) AND (salary#129492 > 3500))\n   +- FileScan csv [Employee#129490,Department#129491,Salary#129492] Batched: false, DataFilters: [isnotnull(Salary#129492), (Salary#129492 > 3500)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/salary_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Salary), GreaterThan(Salary,3500)], ReadSchema: struct<Employee:string,Department:string,Salary:int>\n\n\n"
     ]
    }
   ],
   "source": [
    "# SparkSession setup \n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"CatalystOptimizerExample\").getOrCreate() \n",
    "# Load data \n",
    "df = spark.read.csv(\"/salary_data.csv\", header=True, inferSchema=True) \n",
    "# Query with Catalyst Optimizer \n",
    "result_df = df.select(\"employee\", \"department\").filter(df[\"salary\"] > 3500) \n",
    "# Explain the optimized query plan \n",
    "result_df.explain() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ba28ee-80d0-4210-acb7-4a45bee2815b",
     "showTitle": true,
     "title": "Unpersisting Data"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Employee: string, Department: string, Salary: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache a DataFrame \n",
    "df.cache() \n",
    "# Unpersist the cached DataFrame \n",
    "df.unpersist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25bff695-c206-4800-80c0-e6dde6962438",
     "showTitle": true,
     "title": "Repartitioning Data"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Employee: string, Department: string, Salary: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition a DataFrame into 8 partitions \n",
    "df.repartition(8) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb87f1e-ba2d-47fb-a7c1-b74e1f293598",
     "showTitle": true,
     "title": "Coalescing Data"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Employee: string, Department: string, Salary: int]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coalesce a DataFrame to 4 partitions \n",
    "df.coalesce(4) \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 969987236417588,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Chapter 5 Code",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
